{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpLyMx_TE3dz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "from random import shuffle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2646,
     "status": "ok",
     "timestamp": 1560669729803,
     "user": {
      "displayName": "Мухаммад Назиров",
      "photoUrl": "https://lh5.googleusercontent.com/-pGon_rPbykw/AAAAAAAAAAI/AAAAAAAAAgU/Fzvhl3aZXqU/s64/photo.jpg",
      "userId": "15764388686524151070"
     },
     "user_tz": -180
    },
    "id": "hJdvUklVjFT1",
    "outputId": "4d379782-2a27-465b-eb16-e8a72b555786"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    With Luke Perry gone, our idols are getting ol...\n",
       "1    Science says you should be careful how you use...\n",
       "2    Royal Family's new social media guidelines wan...\n",
       "3    Facebook prepares to push Oculus VR headsets f...\n",
       "4    Shazam gets unexpected help from Batman in new...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('data.json')[0]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU8K945zjXnE"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.char2idx = {}\n",
    "        self.idx2char = []\n",
    "\n",
    "    def add_char(self, char):\n",
    "        if char not in self.char2idx:\n",
    "            self.idx2char.append(char)\n",
    "            self.char2idx[char] = len(self.idx2char) - 1\n",
    "        return self.char2idx[char]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZceU9l-LXnV"
   },
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yKeb9OcMMgq"
   },
   "outputs": [],
   "source": [
    "dictry = Dictionary()\n",
    "def fill_dict(title: str):\n",
    "    chars = list(title)\n",
    "    for char in chars:\n",
    "        dictry.add_char(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W4nlWWphNzG4"
   },
   "outputs": [],
   "source": [
    "data.apply(lambda title: fill_dict(title))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 650,
     "status": "ok",
     "timestamp": 1560676229644,
     "user": {
      "displayName": "Мухаммад Назиров",
      "photoUrl": "https://lh5.googleusercontent.com/-pGon_rPbykw/AAAAAAAAAAI/AAAAAAAAAgU/Fzvhl3aZXqU/s64/photo.jpg",
      "userId": "15764388686524151070"
     },
     "user_tz": -180
    },
    "id": "zenhcG83OAkU",
    "outputId": "403b48e4-395b-4209-acb4-d92c6556f086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('~' in dictry.char2idx)\n",
    "# '~' will denote the end of a sequence\n",
    "dictry.add_char('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lD5PaKmXOW8S"
   },
   "outputs": [],
   "source": [
    "def tokenize(title):\n",
    "    chars = list(title)\n",
    "    chars.append('~')\n",
    "    tokenized = list(map(lambda char: dictry.char2idx[char], chars))\n",
    "    return tokenized\n",
    "  \n",
    "tokenizedData = data.apply(tokenize)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainData, valData = train_test_split(tokenizedData, test_size=0.01)\n",
    "\n",
    "trainData = trainData.tolist()\n",
    "valData = valData.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HO5yvyWikRE8"
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "def construct_batch(source, i):\n",
    "    begin = i * batch_size\n",
    "    if begin >= len(source): return None, None\n",
    "    end = min([len(source), begin + batch_size])\n",
    "    vecs = source[begin:end]\n",
    "    len_vec = max([len(title) for title in vecs])\n",
    "    for vec in vecs:\n",
    "        vec.extend([dictry.char2idx['~']] * (len_vec - len(vec)))\n",
    "    x = [torch.tensor(vec, dtype=torch.long).to(device) for vec in vecs]\n",
    "    y = [torch.tensor([*vec[1:], dictry.char2idx['~']], dtype=torch.long).to(device) for vec in vecs]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, emsize, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, emsize)\n",
    "        self.lstm = nn.LSTM(emsize, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input):\n",
    "        emb = self.drop(torch.cuda.FloatTensor(self.encoder(input)))\n",
    "        output, _ = self.lstm(emb.view(len(input), 1, -1))\n",
    "        output = self.drop(torch.cuda.FloatTensor(output))\n",
    "        decoded = self.decoder(output.view(len(input), -1))\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(self.nlayers, 1, self.nhid),\n",
    "            torch.zeros(self.nlayers, 1, self.nhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1560681675177,
     "user": {
      "displayName": "Мухаммад Назиров",
      "photoUrl": "https://lh5.googleusercontent.com/-pGon_rPbykw/AAAAAAAAAAI/AAAAAAAAAgU/Fzvhl3aZXqU/s64/photo.jpg",
      "userId": "15764388686524151070"
     },
     "user_tz": -180
    },
    "id": "YTK-qnW6yedW",
    "outputId": "dbe11346-906f-4654-c38e-4396b379f008"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    \n",
    "    print_after_batch_num = 100\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    num_batches = len(trainData) // batch_size\n",
    "    #num_batches = 101\n",
    "\n",
    "    shuffle(trainData)\n",
    "    \n",
    "    for batch in range(0, num_batches):\n",
    "        x, y = construct_batch(trainData, batch)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            model.zero_grad()\n",
    "            output = model(x[i])\n",
    "            loss = criterion(output, y[i])\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if batch % print_after_batch_num == 0 and batch > 0:\n",
    "            cur_loss = total_loss / (batch_size * print_after_batch_num)\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f}'.format(\n",
    "                epoch, batch, num_batches,\n",
    "                elapsed * 1000 / print_after_batch_num, cur_loss))\n",
    "            total_loss = 0\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        num_batches = len(data_source) // batch_size\n",
    "        for batch in range(0, num_batches):\n",
    "            x, y = construct_batch(data_source, batch)\n",
    "            for i in range(batch_size):\n",
    "                output = model(x[i])\n",
    "                total_loss += criterion(output, y[i])\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "ntokens = len(dictry.idx2char)\n",
    "nhid = 256\n",
    "emsize = ntokens\n",
    "nlayers = 1\n",
    "dropout = 0.3\n",
    "device = torch.device(\"cuda\")\n",
    "model = LSTMModel(ntokens, emsize, nhid, nlayers, dropout).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |   100/13284 batches | ms/batch 133.35 | loss  2.36\n",
      "| epoch   0 |   200/13284 batches | ms/batch 263.45 | loss  2.04\n",
      "| epoch   0 |   300/13284 batches | ms/batch 390.66 | loss  1.96\n",
      "| epoch   0 |   400/13284 batches | ms/batch 520.10 | loss  1.84\n",
      "| epoch   0 |   500/13284 batches | ms/batch 648.68 | loss  1.83\n",
      "| epoch   0 |   600/13284 batches | ms/batch 777.25 | loss  1.77\n",
      "| epoch   0 |   700/13284 batches | ms/batch 906.43 | loss  1.72\n",
      "| epoch   0 |   800/13284 batches | ms/batch 1035.53 | loss  1.68\n",
      "| epoch   0 |   900/13284 batches | ms/batch 1165.80 | loss  1.65\n",
      "| epoch   0 |  1000/13284 batches | ms/batch 1290.61 | loss  1.68\n",
      "| epoch   0 |  1100/13284 batches | ms/batch 1422.34 | loss  1.61\n",
      "| epoch   0 |  1200/13284 batches | ms/batch 1546.92 | loss  1.62\n",
      "| epoch   0 |  1300/13284 batches | ms/batch 1674.25 | loss  1.62\n",
      "| epoch   0 |  1400/13284 batches | ms/batch 1803.40 | loss  1.57\n",
      "| epoch   0 |  1500/13284 batches | ms/batch 1934.24 | loss  1.56\n",
      "| epoch   0 |  1600/13284 batches | ms/batch 2063.63 | loss  1.60\n",
      "| epoch   0 |  1700/13284 batches | ms/batch 2192.69 | loss  1.55\n",
      "| epoch   0 |  1800/13284 batches | ms/batch 2322.52 | loss  1.57\n",
      "| epoch   0 |  1900/13284 batches | ms/batch 2448.71 | loss  1.53\n",
      "| epoch   0 |  2000/13284 batches | ms/batch 2578.55 | loss  1.51\n",
      "| epoch   0 |  2100/13284 batches | ms/batch 2706.41 | loss  1.52\n",
      "| epoch   0 |  2200/13284 batches | ms/batch 2836.17 | loss  1.52\n",
      "| epoch   0 |  2300/13284 batches | ms/batch 2962.76 | loss  1.51\n",
      "| epoch   0 |  2400/13284 batches | ms/batch 3089.21 | loss  1.50\n",
      "| epoch   0 |  2500/13284 batches | ms/batch 3218.78 | loss  1.47\n",
      "| epoch   0 |  2600/13284 batches | ms/batch 3344.23 | loss  1.50\n",
      "| epoch   0 |  2700/13284 batches | ms/batch 3471.05 | loss  1.48\n",
      "| epoch   0 |  2800/13284 batches | ms/batch 3598.23 | loss  1.50\n",
      "| epoch   0 |  2900/13284 batches | ms/batch 3724.23 | loss  1.50\n",
      "| epoch   0 |  3000/13284 batches | ms/batch 3850.71 | loss  1.47\n",
      "| epoch   0 |  3100/13284 batches | ms/batch 3977.27 | loss  1.45\n",
      "| epoch   0 |  3200/13284 batches | ms/batch 4102.57 | loss  1.49\n",
      "| epoch   0 |  3300/13284 batches | ms/batch 4228.71 | loss  1.48\n",
      "| epoch   0 |  3400/13284 batches | ms/batch 4354.43 | loss  1.47\n",
      "| epoch   0 |  3500/13284 batches | ms/batch 4480.10 | loss  1.46\n",
      "| epoch   0 |  3600/13284 batches | ms/batch 4604.20 | loss  1.46\n",
      "| epoch   0 |  3700/13284 batches | ms/batch 4729.20 | loss  1.46\n",
      "| epoch   0 |  3800/13284 batches | ms/batch 4855.21 | loss  1.47\n",
      "| epoch   0 |  3900/13284 batches | ms/batch 4980.83 | loss  1.45\n",
      "| epoch   0 |  4000/13284 batches | ms/batch 5108.91 | loss  1.45\n",
      "| epoch   0 |  4100/13284 batches | ms/batch 5236.53 | loss  1.43\n",
      "| epoch   0 |  4200/13284 batches | ms/batch 5360.25 | loss  1.47\n",
      "| epoch   0 |  4300/13284 batches | ms/batch 5488.73 | loss  1.43\n",
      "| epoch   0 |  4400/13284 batches | ms/batch 5615.36 | loss  1.43\n",
      "| epoch   0 |  4500/13284 batches | ms/batch 5740.24 | loss  1.44\n",
      "| epoch   0 |  4600/13284 batches | ms/batch 5867.15 | loss  1.42\n",
      "| epoch   0 |  4700/13284 batches | ms/batch 5993.48 | loss  1.43\n",
      "| epoch   0 |  4800/13284 batches | ms/batch 6120.16 | loss  1.42\n",
      "| epoch   0 |  4900/13284 batches | ms/batch 6246.77 | loss  1.43\n",
      "| epoch   0 |  5000/13284 batches | ms/batch 6370.20 | loss  1.44\n",
      "| epoch   0 |  5100/13284 batches | ms/batch 6496.22 | loss  1.44\n",
      "| epoch   0 |  5200/13284 batches | ms/batch 6621.39 | loss  1.41\n",
      "| epoch   0 |  5300/13284 batches | ms/batch 6747.57 | loss  1.44\n",
      "| epoch   0 |  5400/13284 batches | ms/batch 6874.42 | loss  1.40\n",
      "| epoch   0 |  5500/13284 batches | ms/batch 6998.54 | loss  1.43\n",
      "| epoch   0 |  5600/13284 batches | ms/batch 7125.05 | loss  1.43\n",
      "| epoch   0 |  5700/13284 batches | ms/batch 7251.45 | loss  1.42\n",
      "| epoch   0 |  5800/13284 batches | ms/batch 7380.17 | loss  1.38\n",
      "| epoch   0 |  5900/13284 batches | ms/batch 7507.27 | loss  1.40\n",
      "| epoch   0 |  6000/13284 batches | ms/batch 7631.15 | loss  1.43\n",
      "| epoch   0 |  6100/13284 batches | ms/batch 7760.22 | loss  1.42\n",
      "| epoch   0 |  6200/13284 batches | ms/batch 7889.70 | loss  1.39\n",
      "| epoch   0 |  6300/13284 batches | ms/batch 8016.90 | loss  1.40\n",
      "| epoch   0 |  6400/13284 batches | ms/batch 8142.42 | loss  1.40\n",
      "| epoch   0 |  6500/13284 batches | ms/batch 8269.94 | loss  1.40\n",
      "| epoch   0 |  6600/13284 batches | ms/batch 8399.16 | loss  1.42\n",
      "| epoch   0 |  6700/13284 batches | ms/batch 8528.21 | loss  1.40\n",
      "| epoch   0 |  6800/13284 batches | ms/batch 8657.43 | loss  1.38\n",
      "| epoch   0 |  6900/13284 batches | ms/batch 8782.25 | loss  1.40\n",
      "| epoch   0 |  7000/13284 batches | ms/batch 8911.98 | loss  1.35\n",
      "| epoch   0 |  7100/13284 batches | ms/batch 9037.74 | loss  1.42\n",
      "| epoch   0 |  7200/13284 batches | ms/batch 9163.89 | loss  1.39\n",
      "| epoch   0 |  7300/13284 batches | ms/batch 9290.46 | loss  1.40\n",
      "| epoch   0 |  7400/13284 batches | ms/batch 9416.93 | loss  1.40\n",
      "| epoch   0 |  7500/13284 batches | ms/batch 9543.13 | loss  1.36\n",
      "| epoch   0 |  7600/13284 batches | ms/batch 9668.62 | loss  1.39\n",
      "| epoch   0 |  7700/13284 batches | ms/batch 9794.00 | loss  1.39\n",
      "| epoch   0 |  7800/13284 batches | ms/batch 9921.36 | loss  1.39\n",
      "| epoch   0 |  7900/13284 batches | ms/batch 10047.68 | loss  1.37\n",
      "| epoch   0 |  8000/13284 batches | ms/batch 10172.39 | loss  1.37\n",
      "| epoch   0 |  8100/13284 batches | ms/batch 10298.04 | loss  1.36\n",
      "| epoch   0 |  8200/13284 batches | ms/batch 10422.07 | loss  1.39\n",
      "| epoch   0 |  8300/13284 batches | ms/batch 10546.49 | loss  1.39\n",
      "| epoch   0 |  8400/13284 batches | ms/batch 10669.32 | loss  1.36\n",
      "| epoch   0 |  8500/13284 batches | ms/batch 10792.51 | loss  1.35\n",
      "| epoch   0 |  8600/13284 batches | ms/batch 10914.79 | loss  1.40\n",
      "| epoch   0 |  8700/13284 batches | ms/batch 11043.75 | loss  1.36\n",
      "| epoch   0 |  8800/13284 batches | ms/batch 11172.56 | loss  1.37\n",
      "| epoch   0 |  8900/13284 batches | ms/batch 11299.92 | loss  1.37\n",
      "| epoch   0 |  9000/13284 batches | ms/batch 11425.60 | loss  1.38\n",
      "| epoch   0 |  9100/13284 batches | ms/batch 11551.81 | loss  1.37\n",
      "| epoch   0 |  9200/13284 batches | ms/batch 11678.05 | loss  1.37\n",
      "| epoch   0 |  9300/13284 batches | ms/batch 11803.08 | loss  1.39\n",
      "| epoch   0 |  9400/13284 batches | ms/batch 11928.51 | loss  1.38\n",
      "| epoch   0 |  9500/13284 batches | ms/batch 12055.68 | loss  1.36\n",
      "| epoch   0 |  9600/13284 batches | ms/batch 12184.72 | loss  1.36\n",
      "| epoch   0 |  9700/13284 batches | ms/batch 12311.87 | loss  1.35\n",
      "| epoch   0 |  9800/13284 batches | ms/batch 12438.10 | loss  1.37\n",
      "| epoch   0 |  9900/13284 batches | ms/batch 12562.66 | loss  1.37\n",
      "| epoch   0 | 10000/13284 batches | ms/batch 12688.47 | loss  1.37\n",
      "| epoch   0 | 10100/13284 batches | ms/batch 12813.29 | loss  1.38\n",
      "| epoch   0 | 10200/13284 batches | ms/batch 12933.28 | loss  1.40\n",
      "| epoch   0 | 10300/13284 batches | ms/batch 13057.94 | loss  1.36\n",
      "| epoch   0 | 10400/13284 batches | ms/batch 13180.19 | loss  1.39\n",
      "| epoch   0 | 10500/13284 batches | ms/batch 13306.02 | loss  1.36\n",
      "| epoch   0 | 10600/13284 batches | ms/batch 13432.62 | loss  1.36\n",
      "| epoch   0 | 10700/13284 batches | ms/batch 13559.62 | loss  1.35\n",
      "| epoch   0 | 10800/13284 batches | ms/batch 13685.35 | loss  1.38\n",
      "| epoch   0 | 10900/13284 batches | ms/batch 13811.73 | loss  1.37\n",
      "| epoch   0 | 11000/13284 batches | ms/batch 13937.67 | loss  1.36\n",
      "| epoch   0 | 11100/13284 batches | ms/batch 14064.43 | loss  1.37\n",
      "| epoch   0 | 11200/13284 batches | ms/batch 14190.26 | loss  1.38\n",
      "| epoch   0 | 11300/13284 batches | ms/batch 14317.26 | loss  1.34\n",
      "| epoch   0 | 11400/13284 batches | ms/batch 14442.42 | loss  1.37\n",
      "| epoch   0 | 11500/13284 batches | ms/batch 14569.34 | loss  1.37\n",
      "| epoch   0 | 11600/13284 batches | ms/batch 14695.52 | loss  1.36\n",
      "| epoch   0 | 11700/13284 batches | ms/batch 14821.47 | loss  1.37\n",
      "| epoch   0 | 11800/13284 batches | ms/batch 14947.93 | loss  1.36\n",
      "| epoch   0 | 11900/13284 batches | ms/batch 15075.01 | loss  1.37\n",
      "| epoch   0 | 12000/13284 batches | ms/batch 15201.57 | loss  1.35\n",
      "| epoch   0 | 12100/13284 batches | ms/batch 15329.18 | loss  1.34\n",
      "| epoch   0 | 12200/13284 batches | ms/batch 15455.83 | loss  1.39\n",
      "| epoch   0 | 12300/13284 batches | ms/batch 15581.22 | loss  1.37\n",
      "| epoch   0 | 12400/13284 batches | ms/batch 15706.73 | loss  1.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | 12500/13284 batches | ms/batch 15832.03 | loss  1.36\n",
      "| epoch   0 | 12600/13284 batches | ms/batch 15958.56 | loss  1.33\n",
      "| epoch   0 | 12700/13284 batches | ms/batch 16086.60 | loss  1.34\n",
      "| epoch   0 | 12800/13284 batches | ms/batch 16213.77 | loss  1.31\n",
      "| epoch   0 | 12900/13284 batches | ms/batch 16341.26 | loss  1.38\n",
      "| epoch   0 | 13000/13284 batches | ms/batch 16466.65 | loss  1.36\n",
      "| epoch   0 | 13100/13284 batches | ms/batch 16595.31 | loss  1.33\n",
      "| epoch   0 | 13200/13284 batches | ms/batch 16720.82 | loss  1.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 1689.42s | valid loss  1.20 |\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type LSTMModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/13284 batches | ms/batch 146.52 | loss  1.11\n",
      "| epoch   1 |   200/13284 batches | ms/batch 292.42 | loss  1.11\n",
      "| epoch   1 |   300/13284 batches | ms/batch 438.38 | loss  1.11\n",
      "| epoch   1 |   400/13284 batches | ms/batch 582.20 | loss  1.12\n",
      "| epoch   1 |   500/13284 batches | ms/batch 726.13 | loss  1.13\n",
      "| epoch   1 |   600/13284 batches | ms/batch 872.81 | loss  1.09\n",
      "| epoch   1 |   700/13284 batches | ms/batch 1015.71 | loss  1.13\n",
      "| epoch   1 |   800/13284 batches | ms/batch 1160.31 | loss  1.12\n",
      "| epoch   1 |   900/13284 batches | ms/batch 1305.10 | loss  1.10\n",
      "| epoch   1 |  1000/13284 batches | ms/batch 1449.70 | loss  1.10\n",
      "| epoch   1 |  1100/13284 batches | ms/batch 1596.80 | loss  1.12\n",
      "| epoch   1 |  1200/13284 batches | ms/batch 1741.84 | loss  1.12\n",
      "| epoch   1 |  1300/13284 batches | ms/batch 1887.31 | loss  1.10\n",
      "| epoch   1 |  1400/13284 batches | ms/batch 2031.51 | loss  1.13\n",
      "| epoch   1 |  1500/13284 batches | ms/batch 2177.72 | loss  1.11\n",
      "| epoch   1 |  1600/13284 batches | ms/batch 2323.87 | loss  1.11\n",
      "| epoch   1 |  1700/13284 batches | ms/batch 2470.77 | loss  1.10\n",
      "| epoch   1 |  1800/13284 batches | ms/batch 2616.01 | loss  1.10\n",
      "| epoch   1 |  1900/13284 batches | ms/batch 2761.93 | loss  1.12\n",
      "| epoch   1 |  2000/13284 batches | ms/batch 2906.02 | loss  1.13\n",
      "| epoch   1 |  2100/13284 batches | ms/batch 3052.95 | loss  1.09\n",
      "| epoch   1 |  2200/13284 batches | ms/batch 3195.92 | loss  1.12\n",
      "| epoch   1 |  2300/13284 batches | ms/batch 3341.33 | loss  1.11\n",
      "| epoch   1 |  2400/13284 batches | ms/batch 3486.71 | loss  1.11\n",
      "| epoch   1 |  2500/13284 batches | ms/batch 3632.44 | loss  1.12\n",
      "| epoch   1 |  2600/13284 batches | ms/batch 3780.34 | loss  1.08\n",
      "| epoch   1 |  2700/13284 batches | ms/batch 3926.92 | loss  1.11\n",
      "| epoch   1 |  2800/13284 batches | ms/batch 4072.98 | loss  1.13\n",
      "| epoch   1 |  2900/13284 batches | ms/batch 4218.53 | loss  1.11\n",
      "| epoch   1 |  3000/13284 batches | ms/batch 4363.93 | loss  1.11\n",
      "| epoch   1 |  3100/13284 batches | ms/batch 4509.97 | loss  1.11\n",
      "| epoch   1 |  3200/13284 batches | ms/batch 4654.96 | loss  1.10\n",
      "| epoch   1 |  3300/13284 batches | ms/batch 4801.28 | loss  1.10\n",
      "| epoch   1 |  3400/13284 batches | ms/batch 4949.18 | loss  1.12\n",
      "| epoch   1 |  3500/13284 batches | ms/batch 5097.13 | loss  1.08\n",
      "| epoch   1 |  3600/13284 batches | ms/batch 5241.89 | loss  1.12\n",
      "| epoch   1 |  3700/13284 batches | ms/batch 5387.04 | loss  1.11\n",
      "| epoch   1 |  3800/13284 batches | ms/batch 5533.27 | loss  1.08\n",
      "| epoch   1 |  3900/13284 batches | ms/batch 5676.66 | loss  1.10\n",
      "| epoch   1 |  4000/13284 batches | ms/batch 5813.90 | loss  1.09\n",
      "| epoch   1 |  4100/13284 batches | ms/batch 5955.20 | loss  1.09\n",
      "| epoch   1 |  4200/13284 batches | ms/batch 6097.44 | loss  1.10\n",
      "| epoch   1 |  4300/13284 batches | ms/batch 6238.09 | loss  1.10\n",
      "| epoch   1 |  4400/13284 batches | ms/batch 6379.10 | loss  1.12\n",
      "| epoch   1 |  4500/13284 batches | ms/batch 6523.80 | loss  1.10\n",
      "| epoch   1 |  4600/13284 batches | ms/batch 6665.94 | loss  1.12\n",
      "| epoch   1 |  4700/13284 batches | ms/batch 6809.20 | loss  1.09\n",
      "| epoch   1 |  4800/13284 batches | ms/batch 6955.32 | loss  1.10\n",
      "| epoch   1 |  4900/13284 batches | ms/batch 7099.48 | loss  1.11\n",
      "| epoch   1 |  5000/13284 batches | ms/batch 7245.30 | loss  1.10\n",
      "| epoch   1 |  5100/13284 batches | ms/batch 7390.79 | loss  1.08\n",
      "| epoch   1 |  5200/13284 batches | ms/batch 7536.30 | loss  1.09\n",
      "| epoch   1 |  5300/13284 batches | ms/batch 7679.59 | loss  1.13\n",
      "| epoch   1 |  5400/13284 batches | ms/batch 7822.19 | loss  1.11\n",
      "| epoch   1 |  5500/13284 batches | ms/batch 7964.98 | loss  1.12\n",
      "| epoch   1 |  5600/13284 batches | ms/batch 8112.45 | loss  1.10\n",
      "| epoch   1 |  5700/13284 batches | ms/batch 8258.50 | loss  1.11\n",
      "| epoch   1 |  5800/13284 batches | ms/batch 8406.45 | loss  1.09\n",
      "| epoch   1 |  5900/13284 batches | ms/batch 8550.65 | loss  1.12\n",
      "| epoch   1 |  6000/13284 batches | ms/batch 8695.88 | loss  1.11\n",
      "| epoch   1 |  6100/13284 batches | ms/batch 8840.36 | loss  1.10\n",
      "| epoch   1 |  6200/13284 batches | ms/batch 8984.18 | loss  1.10\n",
      "| epoch   1 |  6300/13284 batches | ms/batch 9126.10 | loss  1.11\n",
      "| epoch   1 |  6400/13284 batches | ms/batch 9269.77 | loss  1.09\n",
      "| epoch   1 |  6500/13284 batches | ms/batch 9410.08 | loss  1.09\n",
      "| epoch   1 |  6600/13284 batches | ms/batch 9551.17 | loss  1.12\n",
      "| epoch   1 |  6700/13284 batches | ms/batch 9695.64 | loss  1.09\n",
      "| epoch   1 |  6800/13284 batches | ms/batch 9839.00 | loss  1.10\n",
      "| epoch   1 |  6900/13284 batches | ms/batch 9983.15 | loss  1.09\n",
      "| epoch   1 |  7000/13284 batches | ms/batch 10124.71 | loss  1.09\n",
      "| epoch   1 |  7100/13284 batches | ms/batch 10266.55 | loss  1.09\n",
      "| epoch   1 |  7200/13284 batches | ms/batch 10413.29 | loss  1.07\n",
      "| epoch   1 |  7300/13284 batches | ms/batch 10560.61 | loss  1.08\n",
      "| epoch   1 |  7400/13284 batches | ms/batch 10705.41 | loss  1.11\n",
      "| epoch   1 |  7500/13284 batches | ms/batch 10851.24 | loss  1.09\n",
      "| epoch   1 |  7600/13284 batches | ms/batch 10993.51 | loss  1.12\n",
      "| epoch   1 |  7700/13284 batches | ms/batch 11138.45 | loss  1.09\n",
      "| epoch   1 |  7800/13284 batches | ms/batch 11283.25 | loss  1.09\n",
      "| epoch   1 |  7900/13284 batches | ms/batch 11428.60 | loss  1.11\n",
      "| epoch   1 |  8000/13284 batches | ms/batch 11575.40 | loss  1.10\n",
      "| epoch   1 |  8100/13284 batches | ms/batch 11720.99 | loss  1.09\n",
      "| epoch   1 |  8200/13284 batches | ms/batch 11864.97 | loss  1.12\n",
      "| epoch   1 |  8300/13284 batches | ms/batch 12010.07 | loss  1.07\n",
      "| epoch   1 |  8400/13284 batches | ms/batch 12155.44 | loss  1.08\n",
      "| epoch   1 |  8500/13284 batches | ms/batch 12299.11 | loss  1.10\n",
      "| epoch   1 |  8600/13284 batches | ms/batch 12439.93 | loss  1.10\n",
      "| epoch   1 |  8700/13284 batches | ms/batch 12581.91 | loss  1.12\n",
      "| epoch   1 |  8800/13284 batches | ms/batch 12728.46 | loss  1.08\n",
      "| epoch   1 |  8900/13284 batches | ms/batch 12871.01 | loss  1.09\n",
      "| epoch   1 |  9000/13284 batches | ms/batch 13015.66 | loss  1.09\n",
      "| epoch   1 |  9100/13284 batches | ms/batch 13159.70 | loss  1.10\n",
      "| epoch   1 |  9200/13284 batches | ms/batch 13301.17 | loss  1.10\n",
      "| epoch   1 |  9300/13284 batches | ms/batch 13446.44 | loss  1.09\n",
      "| epoch   1 |  9400/13284 batches | ms/batch 13585.78 | loss  1.12\n",
      "| epoch   1 |  9500/13284 batches | ms/batch 13730.32 | loss  1.08\n",
      "| epoch   1 |  9600/13284 batches | ms/batch 13874.14 | loss  1.11\n",
      "| epoch   1 |  9700/13284 batches | ms/batch 14017.23 | loss  1.11\n",
      "| epoch   1 |  9800/13284 batches | ms/batch 14160.43 | loss  1.09\n",
      "| epoch   1 |  9900/13284 batches | ms/batch 14304.88 | loss  1.08\n",
      "| epoch   1 | 10000/13284 batches | ms/batch 14448.61 | loss  1.12\n",
      "| epoch   1 | 10100/13284 batches | ms/batch 14593.28 | loss  1.09\n",
      "| epoch   1 | 10200/13284 batches | ms/batch 14737.88 | loss  1.11\n",
      "| epoch   1 | 10300/13284 batches | ms/batch 14879.93 | loss  1.11\n",
      "| epoch   1 | 10400/13284 batches | ms/batch 15024.13 | loss  1.09\n",
      "| epoch   1 | 10500/13284 batches | ms/batch 15168.88 | loss  1.08\n",
      "| epoch   1 | 10600/13284 batches | ms/batch 15312.64 | loss  1.07\n",
      "| epoch   1 | 10700/13284 batches | ms/batch 15458.39 | loss  1.09\n",
      "| epoch   1 | 10800/13284 batches | ms/batch 15602.05 | loss  1.07\n",
      "| epoch   1 | 10900/13284 batches | ms/batch 15744.43 | loss  1.08\n",
      "| epoch   1 | 11000/13284 batches | ms/batch 15891.37 | loss  1.06\n",
      "| epoch   1 | 11100/13284 batches | ms/batch 16037.20 | loss  1.09\n",
      "| epoch   1 | 11200/13284 batches | ms/batch 16180.63 | loss  1.10\n",
      "| epoch   1 | 11300/13284 batches | ms/batch 16324.36 | loss  1.09\n",
      "| epoch   1 | 11400/13284 batches | ms/batch 16467.90 | loss  1.10\n",
      "| epoch   1 | 11500/13284 batches | ms/batch 16611.46 | loss  1.10\n",
      "| epoch   1 | 11600/13284 batches | ms/batch 16756.47 | loss  1.09\n",
      "| epoch   1 | 11700/13284 batches | ms/batch 16898.87 | loss  1.10\n",
      "| epoch   1 | 11800/13284 batches | ms/batch 17041.80 | loss  1.10\n",
      "| epoch   1 | 11900/13284 batches | ms/batch 17186.82 | loss  1.08\n",
      "| epoch   1 | 12000/13284 batches | ms/batch 17331.39 | loss  1.08\n",
      "| epoch   1 | 12100/13284 batches | ms/batch 17475.01 | loss  1.09\n",
      "| epoch   1 | 12200/13284 batches | ms/batch 17619.31 | loss  1.08\n",
      "| epoch   1 | 12300/13284 batches | ms/batch 17765.79 | loss  1.07\n",
      "| epoch   1 | 12400/13284 batches | ms/batch 17909.67 | loss  1.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 12500/13284 batches | ms/batch 18053.99 | loss  1.10\n",
      "| epoch   1 | 12600/13284 batches | ms/batch 18200.90 | loss  1.06\n",
      "| epoch   1 | 12700/13284 batches | ms/batch 18344.55 | loss  1.09\n",
      "| epoch   1 | 12800/13284 batches | ms/batch 18486.35 | loss  1.08\n",
      "| epoch   1 | 12900/13284 batches | ms/batch 18629.35 | loss  1.07\n",
      "| epoch   1 | 13000/13284 batches | ms/batch 18771.37 | loss  1.07\n",
      "| epoch   1 | 13100/13284 batches | ms/batch 18914.64 | loss  1.09\n",
      "| epoch   1 | 13200/13284 batches | ms/batch 19058.35 | loss  1.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1923.49s | valid loss  1.16 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/13284 batches | ms/batch 163.43 | loss  0.93\n",
      "| epoch   2 |   200/13284 batches | ms/batch 328.12 | loss  0.92\n",
      "| epoch   2 |   300/13284 batches | ms/batch 491.52 | loss  0.92\n",
      "| epoch   2 |   400/13284 batches | ms/batch 655.58 | loss  0.93\n",
      "| epoch   2 |   500/13284 batches | ms/batch 817.94 | loss  0.91\n",
      "| epoch   2 |   600/13284 batches | ms/batch 979.88 | loss  0.93\n",
      "| epoch   2 |   700/13284 batches | ms/batch 1141.18 | loss  0.94\n",
      "| epoch   2 |   800/13284 batches | ms/batch 1302.30 | loss  0.95\n",
      "| epoch   2 |   900/13284 batches | ms/batch 1465.43 | loss  0.92\n",
      "| epoch   2 |  1000/13284 batches | ms/batch 1628.82 | loss  0.92\n",
      "| epoch   2 |  1100/13284 batches | ms/batch 1792.95 | loss  0.92\n",
      "| epoch   2 |  1200/13284 batches | ms/batch 1954.28 | loss  0.91\n",
      "| epoch   2 |  1300/13284 batches | ms/batch 2117.39 | loss  0.91\n",
      "| epoch   2 |  1400/13284 batches | ms/batch 2280.39 | loss  0.91\n",
      "| epoch   2 |  1500/13284 batches | ms/batch 2441.81 | loss  0.95\n",
      "| epoch   2 |  1600/13284 batches | ms/batch 2603.42 | loss  0.95\n",
      "| epoch   2 |  1700/13284 batches | ms/batch 2765.78 | loss  0.92\n",
      "| epoch   2 |  1800/13284 batches | ms/batch 2930.35 | loss  0.92\n",
      "| epoch   2 |  1900/13284 batches | ms/batch 3095.42 | loss  0.91\n",
      "| epoch   2 |  2000/13284 batches | ms/batch 3258.27 | loss  0.93\n",
      "| epoch   2 |  2100/13284 batches | ms/batch 3421.95 | loss  0.91\n",
      "| epoch   2 |  2200/13284 batches | ms/batch 3584.99 | loss  0.93\n",
      "| epoch   2 |  2300/13284 batches | ms/batch 3742.30 | loss  0.95\n",
      "| epoch   2 |  2400/13284 batches | ms/batch 3902.84 | loss  0.94\n",
      "| epoch   2 |  2500/13284 batches | ms/batch 4065.27 | loss  0.93\n",
      "| epoch   2 |  2600/13284 batches | ms/batch 4223.81 | loss  0.94\n",
      "| epoch   2 |  2700/13284 batches | ms/batch 4384.78 | loss  0.91\n",
      "| epoch   2 |  2800/13284 batches | ms/batch 4545.97 | loss  0.92\n",
      "| epoch   2 |  2900/13284 batches | ms/batch 4706.66 | loss  0.92\n",
      "| epoch   2 |  3000/13284 batches | ms/batch 4866.04 | loss  0.93\n",
      "| epoch   2 |  3100/13284 batches | ms/batch 5027.20 | loss  0.92\n",
      "| epoch   2 |  3200/13284 batches | ms/batch 5187.49 | loss  0.93\n",
      "| epoch   2 |  3300/13284 batches | ms/batch 5348.91 | loss  0.92\n",
      "| epoch   2 |  3400/13284 batches | ms/batch 5511.60 | loss  0.92\n",
      "| epoch   2 |  3500/13284 batches | ms/batch 5675.92 | loss  0.92\n",
      "| epoch   2 |  3600/13284 batches | ms/batch 5837.52 | loss  0.93\n",
      "| epoch   2 |  3700/13284 batches | ms/batch 5997.64 | loss  0.91\n",
      "| epoch   2 |  3800/13284 batches | ms/batch 6152.89 | loss  0.93\n",
      "| epoch   2 |  3900/13284 batches | ms/batch 6312.91 | loss  0.91\n",
      "| epoch   2 |  4000/13284 batches | ms/batch 6473.17 | loss  0.93\n",
      "| epoch   2 |  4100/13284 batches | ms/batch 6631.99 | loss  0.91\n",
      "| epoch   2 |  4200/13284 batches | ms/batch 6792.67 | loss  0.92\n",
      "| epoch   2 |  4300/13284 batches | ms/batch 6950.89 | loss  0.93\n",
      "| epoch   2 |  4400/13284 batches | ms/batch 7110.22 | loss  0.91\n",
      "| epoch   2 |  4500/13284 batches | ms/batch 7273.59 | loss  0.93\n",
      "| epoch   2 |  4600/13284 batches | ms/batch 7437.51 | loss  0.91\n",
      "| epoch   2 |  4700/13284 batches | ms/batch 7600.32 | loss  0.89\n",
      "| epoch   2 |  4800/13284 batches | ms/batch 7764.30 | loss  0.92\n",
      "| epoch   2 |  4900/13284 batches | ms/batch 7924.67 | loss  0.93\n",
      "| epoch   2 |  5000/13284 batches | ms/batch 8086.39 | loss  0.93\n",
      "| epoch   2 |  5100/13284 batches | ms/batch 8247.86 | loss  0.94\n",
      "| epoch   2 |  5200/13284 batches | ms/batch 8408.20 | loss  0.92\n",
      "| epoch   2 |  5300/13284 batches | ms/batch 8567.44 | loss  0.92\n",
      "| epoch   2 |  5400/13284 batches | ms/batch 8728.43 | loss  0.91\n",
      "| epoch   2 |  5500/13284 batches | ms/batch 8888.16 | loss  0.93\n",
      "| epoch   2 |  5600/13284 batches | ms/batch 9049.83 | loss  0.92\n",
      "| epoch   2 |  5700/13284 batches | ms/batch 9210.36 | loss  0.92\n",
      "| epoch   2 |  5800/13284 batches | ms/batch 9373.59 | loss  0.92\n",
      "| epoch   2 |  5900/13284 batches | ms/batch 9536.60 | loss  0.90\n",
      "| epoch   2 |  6000/13284 batches | ms/batch 9700.33 | loss  0.91\n",
      "| epoch   2 |  6100/13284 batches | ms/batch 9862.21 | loss  0.92\n",
      "| epoch   2 |  6200/13284 batches | ms/batch 10024.65 | loss  0.92\n",
      "| epoch   2 |  6300/13284 batches | ms/batch 10185.64 | loss  0.91\n",
      "| epoch   2 |  6400/13284 batches | ms/batch 10350.28 | loss  0.91\n",
      "| epoch   2 |  6500/13284 batches | ms/batch 10510.32 | loss  0.91\n",
      "| epoch   2 |  6600/13284 batches | ms/batch 10675.39 | loss  0.89\n",
      "| epoch   2 |  6700/13284 batches | ms/batch 10835.88 | loss  0.93\n",
      "| epoch   2 |  6800/13284 batches | ms/batch 10996.34 | loss  0.94\n",
      "| epoch   2 |  6900/13284 batches | ms/batch 11158.90 | loss  0.91\n",
      "| epoch   2 |  7000/13284 batches | ms/batch 11322.54 | loss  0.89\n",
      "| epoch   2 |  7100/13284 batches | ms/batch 11486.19 | loss  0.92\n",
      "| epoch   2 |  7200/13284 batches | ms/batch 11650.29 | loss  0.92\n",
      "| epoch   2 |  7300/13284 batches | ms/batch 11811.79 | loss  0.91\n",
      "| epoch   2 |  7400/13284 batches | ms/batch 11972.65 | loss  0.92\n",
      "| epoch   2 |  7500/13284 batches | ms/batch 12136.57 | loss  0.88\n",
      "| epoch   2 |  7600/13284 batches | ms/batch 12298.45 | loss  0.90\n",
      "| epoch   2 |  7700/13284 batches | ms/batch 12462.01 | loss  0.91\n",
      "| epoch   2 |  7800/13284 batches | ms/batch 12625.35 | loss  0.93\n",
      "| epoch   2 |  7900/13284 batches | ms/batch 12787.83 | loss  0.92\n",
      "| epoch   2 |  8000/13284 batches | ms/batch 12951.43 | loss  0.91\n",
      "| epoch   2 |  8100/13284 batches | ms/batch 13114.84 | loss  0.93\n",
      "| epoch   2 |  8200/13284 batches | ms/batch 13275.83 | loss  0.93\n",
      "| epoch   2 |  8300/13284 batches | ms/batch 13438.48 | loss  0.92\n",
      "| epoch   2 |  8400/13284 batches | ms/batch 13601.42 | loss  0.93\n",
      "| epoch   2 |  8500/13284 batches | ms/batch 13764.06 | loss  0.91\n",
      "| epoch   2 |  8600/13284 batches | ms/batch 13924.65 | loss  0.92\n",
      "| epoch   2 |  8700/13284 batches | ms/batch 14087.81 | loss  0.91\n",
      "| epoch   2 |  8800/13284 batches | ms/batch 14249.00 | loss  0.91\n",
      "| epoch   2 |  8900/13284 batches | ms/batch 14411.90 | loss  0.93\n",
      "| epoch   2 |  9000/13284 batches | ms/batch 14574.58 | loss  0.92\n",
      "| epoch   2 |  9100/13284 batches | ms/batch 14735.81 | loss  0.93\n",
      "| epoch   2 |  9200/13284 batches | ms/batch 14898.19 | loss  0.94\n",
      "| epoch   2 |  9300/13284 batches | ms/batch 15058.97 | loss  0.92\n",
      "| epoch   2 |  9400/13284 batches | ms/batch 15221.07 | loss  0.90\n",
      "| epoch   2 |  9500/13284 batches | ms/batch 15382.81 | loss  0.92\n",
      "| epoch   2 |  9600/13284 batches | ms/batch 15547.15 | loss  0.89\n",
      "| epoch   2 |  9700/13284 batches | ms/batch 15710.28 | loss  0.90\n",
      "| epoch   2 |  9800/13284 batches | ms/batch 15873.06 | loss  0.91\n",
      "| epoch   2 |  9900/13284 batches | ms/batch 16028.67 | loss  0.93\n",
      "| epoch   2 | 10000/13284 batches | ms/batch 16187.34 | loss  0.92\n",
      "| epoch   2 | 10100/13284 batches | ms/batch 16346.13 | loss  0.92\n",
      "| epoch   2 | 10200/13284 batches | ms/batch 16506.77 | loss  0.89\n",
      "| epoch   2 | 10300/13284 batches | ms/batch 16668.60 | loss  0.91\n",
      "| epoch   2 | 10400/13284 batches | ms/batch 16828.66 | loss  0.92\n",
      "| epoch   2 | 10500/13284 batches | ms/batch 16990.00 | loss  0.92\n",
      "| epoch   2 | 10600/13284 batches | ms/batch 17152.55 | loss  0.91\n",
      "| epoch   2 | 10700/13284 batches | ms/batch 17313.82 | loss  0.92\n",
      "| epoch   2 | 10800/13284 batches | ms/batch 17476.23 | loss  0.92\n",
      "| epoch   2 | 10900/13284 batches | ms/batch 17638.59 | loss  0.91\n",
      "| epoch   2 | 11000/13284 batches | ms/batch 17800.31 | loss  0.93\n",
      "| epoch   2 | 11100/13284 batches | ms/batch 17963.13 | loss  0.90\n",
      "| epoch   2 | 11200/13284 batches | ms/batch 18123.60 | loss  0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | 11300/13284 batches | ms/batch 18286.47 | loss  0.92\n",
      "| epoch   2 | 11400/13284 batches | ms/batch 18449.22 | loss  0.92\n",
      "| epoch   2 | 11500/13284 batches | ms/batch 18612.25 | loss  0.90\n",
      "| epoch   2 | 11600/13284 batches | ms/batch 18774.74 | loss  0.91\n",
      "| epoch   2 | 11700/13284 batches | ms/batch 18936.29 | loss  0.90\n",
      "| epoch   2 | 11800/13284 batches | ms/batch 19097.40 | loss  0.91\n",
      "| epoch   2 | 11900/13284 batches | ms/batch 19260.76 | loss  0.91\n",
      "| epoch   2 | 12000/13284 batches | ms/batch 19424.92 | loss  0.90\n",
      "| epoch   2 | 12100/13284 batches | ms/batch 19584.05 | loss  0.93\n",
      "| epoch   2 | 12200/13284 batches | ms/batch 19745.19 | loss  0.92\n",
      "| epoch   2 | 12300/13284 batches | ms/batch 19906.71 | loss  0.88\n",
      "| epoch   2 | 12400/13284 batches | ms/batch 20071.23 | loss  0.91\n",
      "| epoch   2 | 12500/13284 batches | ms/batch 20232.94 | loss  0.90\n",
      "| epoch   2 | 12600/13284 batches | ms/batch 20395.96 | loss  0.90\n",
      "| epoch   2 | 12700/13284 batches | ms/batch 20555.45 | loss  0.92\n",
      "| epoch   2 | 12800/13284 batches | ms/batch 20718.85 | loss  0.91\n",
      "| epoch   2 | 12900/13284 batches | ms/batch 20880.87 | loss  0.91\n",
      "| epoch   2 | 13000/13284 batches | ms/batch 21040.31 | loss  0.92\n",
      "| epoch   2 | 13100/13284 batches | ms/batch 21200.58 | loss  0.91\n",
      "| epoch   2 | 13200/13284 batches | ms/batch 21363.54 | loss  0.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 2154.50s | valid loss  1.14 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/13284 batches | ms/batch 180.56 | loss  0.82\n",
      "| epoch   3 |   200/13284 batches | ms/batch 360.56 | loss  0.82\n",
      "| epoch   3 |   300/13284 batches | ms/batch 541.76 | loss  0.79\n",
      "| epoch   3 |   400/13284 batches | ms/batch 720.72 | loss  0.83\n",
      "| epoch   3 |   500/13284 batches | ms/batch 901.45 | loss  0.82\n",
      "| epoch   3 |   600/13284 batches | ms/batch 1082.77 | loss  0.80\n",
      "| epoch   3 |   700/13284 batches | ms/batch 1263.20 | loss  0.80\n",
      "| epoch   3 |   800/13284 batches | ms/batch 1444.21 | loss  0.81\n",
      "| epoch   3 |   900/13284 batches | ms/batch 1625.09 | loss  0.83\n",
      "| epoch   3 |  1000/13284 batches | ms/batch 1804.51 | loss  0.82\n",
      "| epoch   3 |  1100/13284 batches | ms/batch 1984.14 | loss  0.82\n",
      "| epoch   3 |  1200/13284 batches | ms/batch 2152.38 | loss  0.82\n",
      "| epoch   3 |  1300/13284 batches | ms/batch 2329.42 | loss  0.82\n",
      "| epoch   3 |  1400/13284 batches | ms/batch 2505.38 | loss  0.81\n",
      "| epoch   3 |  1500/13284 batches | ms/batch 2684.09 | loss  0.80\n",
      "| epoch   3 |  1600/13284 batches | ms/batch 2862.64 | loss  0.81\n",
      "| epoch   3 |  1700/13284 batches | ms/batch 3041.05 | loss  0.82\n",
      "| epoch   3 |  1800/13284 batches | ms/batch 3218.32 | loss  0.83\n",
      "| epoch   3 |  1900/13284 batches | ms/batch 3399.11 | loss  0.80\n",
      "| epoch   3 |  2000/13284 batches | ms/batch 3578.83 | loss  0.82\n",
      "| epoch   3 |  2100/13284 batches | ms/batch 3759.47 | loss  0.82\n",
      "| epoch   3 |  2200/13284 batches | ms/batch 3940.45 | loss  0.82\n",
      "| epoch   3 |  2300/13284 batches | ms/batch 4121.81 | loss  0.82\n",
      "| epoch   3 |  2400/13284 batches | ms/batch 4300.31 | loss  0.80\n",
      "| epoch   3 |  2500/13284 batches | ms/batch 4479.34 | loss  0.81\n",
      "| epoch   3 |  2600/13284 batches | ms/batch 4660.04 | loss  0.82\n",
      "| epoch   3 |  2700/13284 batches | ms/batch 4837.17 | loss  0.80\n",
      "| epoch   3 |  2800/13284 batches | ms/batch 5015.35 | loss  0.80\n",
      "| epoch   3 |  2900/13284 batches | ms/batch 5193.36 | loss  0.80\n",
      "| epoch   3 |  3000/13284 batches | ms/batch 5373.42 | loss  0.81\n",
      "| epoch   3 |  3100/13284 batches | ms/batch 5552.72 | loss  0.83\n",
      "| epoch   3 |  3200/13284 batches | ms/batch 5731.74 | loss  0.82\n",
      "| epoch   3 |  3300/13284 batches | ms/batch 5912.00 | loss  0.81\n",
      "| epoch   3 |  3400/13284 batches | ms/batch 6092.27 | loss  0.81\n",
      "| epoch   3 |  3500/13284 batches | ms/batch 6271.24 | loss  0.83\n",
      "| epoch   3 |  3600/13284 batches | ms/batch 6452.45 | loss  0.81\n",
      "| epoch   3 |  3700/13284 batches | ms/batch 6633.24 | loss  0.80\n",
      "| epoch   3 |  3800/13284 batches | ms/batch 6813.17 | loss  0.82\n",
      "| epoch   3 |  3900/13284 batches | ms/batch 6992.88 | loss  0.82\n",
      "| epoch   3 |  4000/13284 batches | ms/batch 7172.52 | loss  0.81\n",
      "| epoch   3 |  4100/13284 batches | ms/batch 7352.06 | loss  0.81\n",
      "| epoch   3 |  4200/13284 batches | ms/batch 7532.84 | loss  0.80\n",
      "| epoch   3 |  4300/13284 batches | ms/batch 7713.42 | loss  0.80\n",
      "| epoch   3 |  4400/13284 batches | ms/batch 7893.53 | loss  0.80\n",
      "| epoch   3 |  4500/13284 batches | ms/batch 8073.19 | loss  0.81\n",
      "| epoch   3 |  4600/13284 batches | ms/batch 8252.73 | loss  0.82\n",
      "| epoch   3 |  4700/13284 batches | ms/batch 8432.60 | loss  0.81\n",
      "| epoch   3 |  4800/13284 batches | ms/batch 8611.96 | loss  0.81\n",
      "| epoch   3 |  4900/13284 batches | ms/batch 8792.15 | loss  0.81\n",
      "| epoch   3 |  5000/13284 batches | ms/batch 8972.39 | loss  0.80\n",
      "| epoch   3 |  5100/13284 batches | ms/batch 9153.29 | loss  0.81\n",
      "| epoch   3 |  5200/13284 batches | ms/batch 9333.56 | loss  0.79\n",
      "| epoch   3 |  5300/13284 batches | ms/batch 9513.75 | loss  0.81\n",
      "| epoch   3 |  5400/13284 batches | ms/batch 9692.95 | loss  0.80\n",
      "| epoch   3 |  5500/13284 batches | ms/batch 9871.72 | loss  0.81\n",
      "| epoch   3 |  5600/13284 batches | ms/batch 10050.93 | loss  0.80\n",
      "| epoch   3 |  5700/13284 batches | ms/batch 10231.05 | loss  0.78\n",
      "| epoch   3 |  5800/13284 batches | ms/batch 10412.60 | loss  0.81\n",
      "| epoch   3 |  5900/13284 batches | ms/batch 10592.91 | loss  0.81\n",
      "| epoch   3 |  6000/13284 batches | ms/batch 10773.04 | loss  0.81\n",
      "| epoch   3 |  6100/13284 batches | ms/batch 10953.47 | loss  0.81\n",
      "| epoch   3 |  6200/13284 batches | ms/batch 11135.19 | loss  0.80\n",
      "| epoch   3 |  6300/13284 batches | ms/batch 11316.08 | loss  0.82\n",
      "| epoch   3 |  6400/13284 batches | ms/batch 11497.22 | loss  0.80\n",
      "| epoch   3 |  6500/13284 batches | ms/batch 11675.77 | loss  0.80\n",
      "| epoch   3 |  6600/13284 batches | ms/batch 11854.95 | loss  0.80\n",
      "| epoch   3 |  6700/13284 batches | ms/batch 12034.33 | loss  0.80\n",
      "| epoch   3 |  6800/13284 batches | ms/batch 12213.61 | loss  0.79\n",
      "| epoch   3 |  6900/13284 batches | ms/batch 12395.65 | loss  0.81\n",
      "| epoch   3 |  7000/13284 batches | ms/batch 12575.74 | loss  0.79\n",
      "| epoch   3 |  7100/13284 batches | ms/batch 12753.75 | loss  0.83\n",
      "| epoch   3 |  7200/13284 batches | ms/batch 12933.37 | loss  0.80\n",
      "| epoch   3 |  7300/13284 batches | ms/batch 13110.99 | loss  0.83\n",
      "| epoch   3 |  7400/13284 batches | ms/batch 13288.89 | loss  0.81\n",
      "| epoch   3 |  7500/13284 batches | ms/batch 13467.62 | loss  0.81\n",
      "| epoch   3 |  7600/13284 batches | ms/batch 13648.51 | loss  0.79\n",
      "| epoch   3 |  7700/13284 batches | ms/batch 13828.50 | loss  0.81\n",
      "| epoch   3 |  7800/13284 batches | ms/batch 14007.91 | loss  0.83\n",
      "| epoch   3 |  7900/13284 batches | ms/batch 14186.01 | loss  0.81\n",
      "| epoch   3 |  8000/13284 batches | ms/batch 14362.97 | loss  0.80\n",
      "| epoch   3 |  8100/13284 batches | ms/batch 14540.06 | loss  0.80\n",
      "| epoch   3 |  8200/13284 batches | ms/batch 14719.65 | loss  0.80\n",
      "| epoch   3 |  8300/13284 batches | ms/batch 14900.03 | loss  0.81\n",
      "| epoch   3 |  8400/13284 batches | ms/batch 15077.98 | loss  0.81\n",
      "| epoch   3 |  8500/13284 batches | ms/batch 15257.52 | loss  0.82\n",
      "| epoch   3 |  8600/13284 batches | ms/batch 15435.75 | loss  0.82\n",
      "| epoch   3 |  8700/13284 batches | ms/batch 15616.79 | loss  0.81\n",
      "| epoch   3 |  8800/13284 batches | ms/batch 15798.33 | loss  0.79\n",
      "| epoch   3 |  8900/13284 batches | ms/batch 15978.11 | loss  0.80\n",
      "| epoch   3 |  9000/13284 batches | ms/batch 16156.66 | loss  0.82\n",
      "| epoch   3 |  9100/13284 batches | ms/batch 16339.56 | loss  0.82\n",
      "| epoch   3 |  9200/13284 batches | ms/batch 16522.36 | loss  0.81\n",
      "| epoch   3 |  9300/13284 batches | ms/batch 16701.56 | loss  0.81\n",
      "| epoch   3 |  9400/13284 batches | ms/batch 16879.14 | loss  0.83\n",
      "| epoch   3 |  9500/13284 batches | ms/batch 17058.26 | loss  0.82\n",
      "| epoch   3 |  9600/13284 batches | ms/batch 17236.96 | loss  0.82\n",
      "| epoch   3 |  9700/13284 batches | ms/batch 17416.50 | loss  0.81\n",
      "| epoch   3 |  9800/13284 batches | ms/batch 17595.83 | loss  0.82\n",
      "| epoch   3 |  9900/13284 batches | ms/batch 17775.96 | loss  0.80\n",
      "| epoch   3 | 10000/13284 batches | ms/batch 17956.78 | loss  0.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | 10100/13284 batches | ms/batch 18136.74 | loss  0.82\n",
      "| epoch   3 | 10200/13284 batches | ms/batch 18315.91 | loss  0.83\n",
      "| epoch   3 | 10300/13284 batches | ms/batch 18495.05 | loss  0.81\n",
      "| epoch   3 | 10400/13284 batches | ms/batch 18671.67 | loss  0.81\n",
      "| epoch   3 | 10500/13284 batches | ms/batch 18848.50 | loss  0.83\n",
      "| epoch   3 | 10600/13284 batches | ms/batch 19027.08 | loss  0.82\n",
      "| epoch   3 | 10700/13284 batches | ms/batch 19204.87 | loss  0.81\n",
      "| epoch   3 | 10800/13284 batches | ms/batch 19384.17 | loss  0.81\n",
      "| epoch   3 | 10900/13284 batches | ms/batch 19563.94 | loss  0.82\n",
      "| epoch   3 | 11000/13284 batches | ms/batch 19743.04 | loss  0.81\n",
      "| epoch   3 | 11100/13284 batches | ms/batch 19922.59 | loss  0.82\n",
      "| epoch   3 | 11200/13284 batches | ms/batch 20100.88 | loss  0.80\n",
      "| epoch   3 | 11300/13284 batches | ms/batch 20279.74 | loss  0.81\n",
      "| epoch   3 | 11400/13284 batches | ms/batch 20459.41 | loss  0.81\n",
      "| epoch   3 | 11500/13284 batches | ms/batch 20638.15 | loss  0.82\n",
      "| epoch   3 | 11600/13284 batches | ms/batch 20817.63 | loss  0.81\n",
      "| epoch   3 | 11700/13284 batches | ms/batch 20996.24 | loss  0.80\n",
      "| epoch   3 | 11800/13284 batches | ms/batch 21175.80 | loss  0.82\n",
      "| epoch   3 | 11900/13284 batches | ms/batch 21354.18 | loss  0.82\n",
      "| epoch   3 | 12000/13284 batches | ms/batch 21535.29 | loss  0.80\n",
      "| epoch   3 | 12100/13284 batches | ms/batch 21714.09 | loss  0.81\n",
      "| epoch   3 | 12200/13284 batches | ms/batch 21894.39 | loss  0.81\n",
      "| epoch   3 | 12300/13284 batches | ms/batch 22075.30 | loss  0.80\n",
      "| epoch   3 | 12400/13284 batches | ms/batch 22252.69 | loss  0.81\n",
      "| epoch   3 | 12500/13284 batches | ms/batch 22430.49 | loss  0.81\n",
      "| epoch   3 | 12600/13284 batches | ms/batch 22609.17 | loss  0.80\n",
      "| epoch   3 | 12700/13284 batches | ms/batch 22788.31 | loss  0.81\n",
      "| epoch   3 | 12800/13284 batches | ms/batch 22972.22 | loss  0.81\n",
      "| epoch   3 | 12900/13284 batches | ms/batch 23155.67 | loss  0.83\n",
      "| epoch   3 | 13000/13284 batches | ms/batch 23337.54 | loss  0.79\n",
      "| epoch   3 | 13100/13284 batches | ms/batch 23515.06 | loss  0.81\n",
      "| epoch   3 | 13200/13284 batches | ms/batch 23694.16 | loss  0.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 2389.56s | valid loss  1.13 |\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/13284 batches | ms/batch 186.92 | loss  0.77\n",
      "| epoch   4 |   200/13284 batches | ms/batch 373.93 | loss  0.76\n",
      "| epoch   4 |   300/13284 batches | ms/batch 560.67 | loss  0.75\n",
      "| epoch   4 |   400/13284 batches | ms/batch 747.47 | loss  0.76\n",
      "| epoch   4 |   500/13284 batches | ms/batch 930.78 | loss  0.75\n",
      "| epoch   4 |   600/13284 batches | ms/batch 1116.11 | loss  0.76\n",
      "| epoch   4 |   700/13284 batches | ms/batch 1302.09 | loss  0.76\n",
      "| epoch   4 |   800/13284 batches | ms/batch 1487.37 | loss  0.76\n",
      "| epoch   4 |   900/13284 batches | ms/batch 1672.28 | loss  0.75\n",
      "| epoch   4 |  1000/13284 batches | ms/batch 1857.88 | loss  0.78\n",
      "| epoch   4 |  1100/13284 batches | ms/batch 2042.58 | loss  0.77\n",
      "| epoch   4 |  1200/13284 batches | ms/batch 2227.95 | loss  0.77\n",
      "| epoch   4 |  1300/13284 batches | ms/batch 2413.44 | loss  0.75\n",
      "| epoch   4 |  1400/13284 batches | ms/batch 2599.18 | loss  0.76\n",
      "| epoch   4 |  1500/13284 batches | ms/batch 2785.52 | loss  0.76\n",
      "| epoch   4 |  1600/13284 batches | ms/batch 2970.57 | loss  0.76\n",
      "| epoch   4 |  1700/13284 batches | ms/batch 3155.68 | loss  0.77\n",
      "| epoch   4 |  1800/13284 batches | ms/batch 3340.12 | loss  0.75\n",
      "| epoch   4 |  1900/13284 batches | ms/batch 3526.70 | loss  0.75\n",
      "| epoch   4 |  2000/13284 batches | ms/batch 3715.24 | loss  0.75\n",
      "| epoch   4 |  2100/13284 batches | ms/batch 3903.59 | loss  0.77\n",
      "| epoch   4 |  2200/13284 batches | ms/batch 4093.93 | loss  0.76\n",
      "| epoch   4 |  2300/13284 batches | ms/batch 4283.84 | loss  0.75\n",
      "| epoch   4 |  2400/13284 batches | ms/batch 4470.75 | loss  0.77\n",
      "| epoch   4 |  2500/13284 batches | ms/batch 4655.80 | loss  0.76\n",
      "| epoch   4 |  2600/13284 batches | ms/batch 4844.37 | loss  0.75\n",
      "| epoch   4 |  2700/13284 batches | ms/batch 5032.58 | loss  0.77\n",
      "| epoch   4 |  2800/13284 batches | ms/batch 5220.86 | loss  0.76\n",
      "| epoch   4 |  2900/13284 batches | ms/batch 5401.44 | loss  0.76\n",
      "| epoch   4 |  3000/13284 batches | ms/batch 5583.72 | loss  0.75\n",
      "| epoch   4 |  3100/13284 batches | ms/batch 5766.07 | loss  0.76\n",
      "| epoch   4 |  3200/13284 batches | ms/batch 5950.46 | loss  0.77\n",
      "| epoch   4 |  3300/13284 batches | ms/batch 6135.34 | loss  0.74\n",
      "| epoch   4 |  3400/13284 batches | ms/batch 6319.74 | loss  0.76\n",
      "| epoch   4 |  3500/13284 batches | ms/batch 6504.95 | loss  0.77\n",
      "| epoch   4 |  3600/13284 batches | ms/batch 6689.65 | loss  0.77\n",
      "| epoch   4 |  3700/13284 batches | ms/batch 6875.01 | loss  0.75\n",
      "| epoch   4 |  3800/13284 batches | ms/batch 7060.30 | loss  0.76\n",
      "| epoch   4 |  3900/13284 batches | ms/batch 7245.26 | loss  0.76\n",
      "| epoch   4 |  4000/13284 batches | ms/batch 7433.60 | loss  0.75\n",
      "| epoch   4 |  4100/13284 batches | ms/batch 7621.70 | loss  0.75\n",
      "| epoch   4 |  4200/13284 batches | ms/batch 7809.86 | loss  0.75\n",
      "| epoch   4 |  4300/13284 batches | ms/batch 7997.18 | loss  0.76\n",
      "| epoch   4 |  4400/13284 batches | ms/batch 8183.24 | loss  0.75\n",
      "| epoch   4 |  4500/13284 batches | ms/batch 8367.86 | loss  0.76\n",
      "| epoch   4 |  4600/13284 batches | ms/batch 8553.54 | loss  0.76\n",
      "| epoch   4 |  4700/13284 batches | ms/batch 8739.13 | loss  0.76\n",
      "| epoch   4 |  4800/13284 batches | ms/batch 8926.62 | loss  0.75\n",
      "| epoch   4 |  4900/13284 batches | ms/batch 9114.83 | loss  0.76\n",
      "| epoch   4 |  5000/13284 batches | ms/batch 9303.50 | loss  0.74\n",
      "| epoch   4 |  5100/13284 batches | ms/batch 9492.68 | loss  0.75\n",
      "| epoch   4 |  5200/13284 batches | ms/batch 9678.42 | loss  0.75\n",
      "| epoch   4 |  5300/13284 batches | ms/batch 9864.01 | loss  0.76\n",
      "| epoch   4 |  5400/13284 batches | ms/batch 10049.92 | loss  0.76\n",
      "| epoch   4 |  5500/13284 batches | ms/batch 10235.70 | loss  0.77\n",
      "| epoch   4 |  5600/13284 batches | ms/batch 10424.03 | loss  0.74\n",
      "| epoch   4 |  5700/13284 batches | ms/batch 10608.99 | loss  0.77\n",
      "| epoch   4 |  5800/13284 batches | ms/batch 10795.19 | loss  0.76\n",
      "| epoch   4 |  5900/13284 batches | ms/batch 10980.14 | loss  0.76\n",
      "| epoch   4 |  6000/13284 batches | ms/batch 11165.51 | loss  0.76\n",
      "| epoch   4 |  6100/13284 batches | ms/batch 11349.20 | loss  0.75\n",
      "| epoch   4 |  6200/13284 batches | ms/batch 11533.46 | loss  0.76\n",
      "| epoch   4 |  6300/13284 batches | ms/batch 11718.54 | loss  0.76\n",
      "| epoch   4 |  6400/13284 batches | ms/batch 11903.80 | loss  0.76\n",
      "| epoch   4 |  6500/13284 batches | ms/batch 12091.29 | loss  0.76\n",
      "| epoch   4 |  6600/13284 batches | ms/batch 12282.49 | loss  0.76\n",
      "| epoch   4 |  6700/13284 batches | ms/batch 12472.00 | loss  0.76\n",
      "| epoch   4 |  6800/13284 batches | ms/batch 12660.94 | loss  0.75\n",
      "| epoch   4 |  6900/13284 batches | ms/batch 12851.03 | loss  0.76\n",
      "| epoch   4 |  7000/13284 batches | ms/batch 13041.15 | loss  0.76\n",
      "| epoch   4 |  7100/13284 batches | ms/batch 13231.80 | loss  0.76\n",
      "| epoch   4 |  7200/13284 batches | ms/batch 13422.50 | loss  0.76\n",
      "| epoch   4 |  7300/13284 batches | ms/batch 13612.60 | loss  0.76\n",
      "| epoch   4 |  7400/13284 batches | ms/batch 13800.51 | loss  0.76\n",
      "| epoch   4 |  7500/13284 batches | ms/batch 13988.00 | loss  0.76\n",
      "| epoch   4 |  7600/13284 batches | ms/batch 14174.20 | loss  0.77\n",
      "| epoch   4 |  7700/13284 batches | ms/batch 14359.80 | loss  0.74\n",
      "| epoch   4 |  7800/13284 batches | ms/batch 14545.18 | loss  0.76\n",
      "| epoch   4 |  7900/13284 batches | ms/batch 14731.00 | loss  0.77\n",
      "| epoch   4 |  8000/13284 batches | ms/batch 14916.43 | loss  0.76\n",
      "| epoch   4 |  8100/13284 batches | ms/batch 15101.14 | loss  0.74\n",
      "| epoch   4 |  8200/13284 batches | ms/batch 15285.87 | loss  0.76\n",
      "| epoch   4 |  8300/13284 batches | ms/batch 15471.33 | loss  0.76\n",
      "| epoch   4 |  8400/13284 batches | ms/batch 15654.51 | loss  0.76\n",
      "| epoch   4 |  8500/13284 batches | ms/batch 15838.86 | loss  0.76\n",
      "| epoch   4 |  8600/13284 batches | ms/batch 16023.56 | loss  0.76\n",
      "| epoch   4 |  8700/13284 batches | ms/batch 16206.66 | loss  0.77\n",
      "| epoch   4 |  8800/13284 batches | ms/batch 16389.55 | loss  0.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |  8900/13284 batches | ms/batch 16574.08 | loss  0.77\n",
      "| epoch   4 |  9000/13284 batches | ms/batch 16759.63 | loss  0.75\n",
      "| epoch   4 |  9100/13284 batches | ms/batch 16943.75 | loss  0.76\n",
      "| epoch   4 |  9200/13284 batches | ms/batch 17127.74 | loss  0.76\n",
      "| epoch   4 |  9300/13284 batches | ms/batch 17313.51 | loss  0.77\n",
      "| epoch   4 |  9400/13284 batches | ms/batch 17498.38 | loss  0.75\n",
      "| epoch   4 |  9500/13284 batches | ms/batch 17683.66 | loss  0.74\n",
      "| epoch   4 |  9600/13284 batches | ms/batch 17870.96 | loss  0.76\n",
      "| epoch   4 |  9700/13284 batches | ms/batch 18058.63 | loss  0.77\n",
      "| epoch   4 |  9800/13284 batches | ms/batch 18244.31 | loss  0.75\n",
      "| epoch   4 |  9900/13284 batches | ms/batch 18429.66 | loss  0.76\n",
      "| epoch   4 | 10000/13284 batches | ms/batch 18615.36 | loss  0.77\n",
      "| epoch   4 | 10100/13284 batches | ms/batch 18802.16 | loss  0.76\n",
      "| epoch   4 | 10200/13284 batches | ms/batch 18988.50 | loss  0.77\n",
      "| epoch   4 | 10300/13284 batches | ms/batch 19176.03 | loss  0.76\n",
      "| epoch   4 | 10400/13284 batches | ms/batch 19362.65 | loss  0.75\n",
      "| epoch   4 | 10500/13284 batches | ms/batch 19548.05 | loss  0.75\n",
      "| epoch   4 | 10600/13284 batches | ms/batch 19734.68 | loss  0.76\n",
      "| epoch   4 | 10700/13284 batches | ms/batch 19921.33 | loss  0.75\n",
      "| epoch   4 | 10800/13284 batches | ms/batch 20107.72 | loss  0.75\n",
      "| epoch   4 | 10900/13284 batches | ms/batch 20295.47 | loss  0.76\n",
      "| epoch   4 | 11000/13284 batches | ms/batch 20483.19 | loss  0.75\n",
      "| epoch   4 | 11100/13284 batches | ms/batch 20669.26 | loss  0.75\n",
      "| epoch   4 | 11200/13284 batches | ms/batch 20853.92 | loss  0.75\n",
      "| epoch   4 | 11300/13284 batches | ms/batch 21041.43 | loss  0.77\n",
      "| epoch   4 | 11400/13284 batches | ms/batch 21228.06 | loss  0.75\n",
      "| epoch   4 | 11500/13284 batches | ms/batch 21413.15 | loss  0.75\n",
      "| epoch   4 | 11600/13284 batches | ms/batch 21598.05 | loss  0.76\n",
      "| epoch   4 | 11700/13284 batches | ms/batch 21782.66 | loss  0.75\n",
      "| epoch   4 | 11800/13284 batches | ms/batch 21967.68 | loss  0.76\n",
      "| epoch   4 | 11900/13284 batches | ms/batch 22152.47 | loss  0.75\n",
      "| epoch   4 | 12000/13284 batches | ms/batch 22337.95 | loss  0.76\n",
      "| epoch   4 | 12100/13284 batches | ms/batch 22522.80 | loss  0.77\n",
      "| epoch   4 | 12200/13284 batches | ms/batch 22707.15 | loss  0.76\n",
      "| epoch   4 | 12300/13284 batches | ms/batch 22891.86 | loss  0.75\n",
      "| epoch   4 | 12400/13284 batches | ms/batch 23075.87 | loss  0.75\n",
      "| epoch   4 | 12500/13284 batches | ms/batch 23259.99 | loss  0.76\n",
      "| epoch   4 | 12600/13284 batches | ms/batch 23444.73 | loss  0.76\n",
      "| epoch   4 | 12700/13284 batches | ms/batch 23629.41 | loss  0.77\n",
      "| epoch   4 | 12800/13284 batches | ms/batch 23813.01 | loss  0.73\n",
      "| epoch   4 | 12900/13284 batches | ms/batch 23996.37 | loss  0.74\n",
      "| epoch   4 | 13000/13284 batches | ms/batch 24179.11 | loss  0.75\n",
      "| epoch   4 | 13100/13284 batches | ms/batch 24365.48 | loss  0.76\n",
      "| epoch   4 | 13200/13284 batches | ms/batch 24549.55 | loss  0.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 2476.10s | valid loss  1.12 |\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "best_val_loss = 10000\n",
    "for epoch in range(0, num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        train(epoch)\n",
    "        val_loss = evaluate(valData)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} |'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if val_loss < best_val_loss:\n",
    "            with open(\"lstm_char_level_model.tch\", 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessed.pkl', 'wb') as output:\n",
    "    pickle.dump([dictry, data, trainData, valData], output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('512_preprocessed.pkl', 'rb') as input:\n",
    "    loaded = pickle.load(input)\n",
    "    dictry = loaded[0]\n",
    "    data = loaded[1]\n",
    "    trainData = loaded[2]\n",
    "    valData = loaded[3]\n",
    "with open(\"512_lstm_char_level_model.tch\", 'rb') as f:\n",
    "    model = torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google shows to be a star and star and star and star and star service\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    initial_char = 'G'\n",
    "    ch = initial_char\n",
    "    title = ''\n",
    "    i = 0\n",
    "    while ch != '~':\n",
    "        title += ch\n",
    "        vecs = model(torch.tensor(list(map(lambda x: dictry.char2idx[x], list(title)))).to(device))\n",
    "        ch = dictry.idx2char[torch.argmax(vecs[len(vecs) - 1])]\n",
    "        i += 1\n",
    "    print(title)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
